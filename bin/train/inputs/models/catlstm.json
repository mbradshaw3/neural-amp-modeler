{
    "_comments": [
        "Parametric extension of the LSTM model. All LSTM tips apply plus:",
        " * Make sure that `input_size` is the number of knobs plus one. I've set it",
        "   up like we're modeling a tube screamer (drive/tone/level), so 1+3=4.",
        " * I've messed around with weight decay, but I don't think it's actually",
        "   helpful. Still, it's in there so you can see how to use it if you're",
        "   curious.",
        " * Doesn't seem like the model needs to be all that bigger than the",
        "   non-parametric version, even if you're modeling a fair number of knobs.",
        " * You'll probably have a much larger dataset, so validating every so often ",
        "   in steps instead of epochs helps. Make sure to also set val_check_interval",
        "   under the trainer dict in your learning.json."
    ],
    "net": {
        "name": "CatLSTM",
        "config": {
            "num_layers": 3,
            "hidden_size": 24,
            "train_truncate": 1024,
            "train_burn_in": 4096,
            "input_size": 4
        }
    },
    "loss": {
        "val_loss": "mse",
        "mask_first": 4096
    },
    "optimizer": {
        "lr": 0.01,
        "weight_decay": 1e-09
    },
    "lr_scheduler": {
        "class": "ExponentialLR",
        "kwargs": {
            "gamma": 0.995
        },
        "interval": "step",
        "frequency": 200
    }
}